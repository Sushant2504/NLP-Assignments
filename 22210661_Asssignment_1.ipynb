{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assignment 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform tokenization (Whitesapace, Punctuation-based, Treebank, Tweet, MWE) using NLTK library. Use porter stemmer and snowball stemmer for stemming. Use any technique for lemmatization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whitespace Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['vno3ebr', '3envoibm', 'adeiop', 'vejbga', 'fvr']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "tk = WhitespaceTokenizer()\n",
    "text = \"vno3ebr 3envoibm adeiop vejbga fvr\"\n",
    "\n",
    "txt = tk.tokenize(text)\n",
    "print(txt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given sentence:\n",
      "NLP practical : in the lab 102 .\n",
      "['NLP', 'practical', ':', 'in', 'the', 'lab', '102', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "txt=\"NLP practical : in the lab 102 .\"\n",
    "print(\"Given sentence:\")\n",
    "print(txt)\n",
    "tokenizetext=WhitespaceTokenizer().tokenize(txt)\n",
    "print(tokenizetext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ios sivn nv'sd oviwnr sjnrv'd uvs, svrhn svu?\n",
      "['Ios', 'sivn', 'nv', \"'\", 'sd', 'oviwnr', 'sjnrv', \"'\", 'd', 'uvs', ',', 'svrhn', 'svu', '?']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "WPT = WordPunctTokenizer()\n",
    "\n",
    "Text = \"Ios sivn nv'sd oviwnr sjnrv'd uvs, svrhn svu?\"\n",
    "\n",
    "txt_tokenized = WPT.tokenize(Text)\n",
    "\n",
    "print(Text)\n",
    "print(txt_tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regexp Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Good',\n",
       " 'muffins',\n",
       " 'cost',\n",
       " '$3.88',\n",
       " 'in',\n",
       " 'New',\n",
       " 'York',\n",
       " '.',\n",
       " 'Please',\n",
       " 'buy',\n",
       " 'me',\n",
       " 'two',\n",
       " 'of',\n",
       " 'them',\n",
       " '.',\n",
       " 'Thanks',\n",
       " '.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "s =  \"Good muffins cost $3.88\\nin New York.  Please buy me\\ntwo of them.\\n\\nThanks.\"\n",
    "tokenizer = RegexpTokenizer(r'\\w+|\\$[\\d\\.]+|\\S+')\n",
    "tokenizer.tokenize(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Treebank Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Good',\n",
       " 'muffins',\n",
       " 'cost',\n",
       " '$',\n",
       " '3.88',\n",
       " 'in',\n",
       " 'New',\n",
       " 'York.',\n",
       " 'Please',\n",
       " 'buy',\n",
       " 'me',\n",
       " 'two',\n",
       " 'of',\n",
       " 'them.',\n",
       " 'Thanks',\n",
       " '.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize.treebank import TreebankWordTokenizer\n",
    "\n",
    "s = \"Good muffins cost $3.88\\nin New York.  Please buy me\\ntwo of them.\\nThanks.\"\n",
    "\n",
    "Tbt = TreebankWordTokenizer()\n",
    "\n",
    "Tokenized = Tbt.tokenize(s)\n",
    "Tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MWE Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ivw', 'irnv', 'a_little', 'vsanhr', 'a_little_bit', 'osawmov', 'aeibn', 'a', 'iedl', 'lot', 'tbieb3']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import MWETokenizer\n",
    "\n",
    "mwe = MWETokenizer([('a','little'), ('a','little','bit'),('a','lot')])\n",
    "\n",
    "mwe.add_mwe(('in','spite','of'))\n",
    "print(mwe.tokenize('ivw irnv a little vsanhr a little bit osawmov aeibn a iedl lot tbieb3'.split()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sent Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['isdnrmb ie minh  sn9v8r.', 'hunr4 isvwhj4 svnu4b.', 'sbv8nre busb ijsrnb.']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt_tab')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "text = \"isdnrmb ie minh  sn9v8r. hunr4 isvwhj4 svnu4b. sbv8nre busb ijsrnb.\"\n",
    "sent_tokenize(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "var = \"The shuttlecock is a feathered or (in informal matches) plastic projectile that flies differently from the balls used in many other sports. In particular, the feathers create much higher drag, causing the shuttlecock to decelerate more rapidly. Shuttlecocks also have a high top speed compared to the balls in other racquet sports, making badminton the fastest racquet sport in the world. The flight of the shuttlecock gives the sport its distinctive nature, and in certain languages the sport is named by reference to this feature (e.g., German Federball, literally feather-ball).\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'shuttlecock', 'is', 'a', 'feathered', 'or', '(', 'in', 'informal', 'matches', ')', 'plastic', 'projectile', 'that', 'flies', 'differently', 'from', 'the', 'balls', 'used', 'in', 'many', 'other', 'sports', '.', 'In', 'particular', ',', 'the', 'feathers', 'create', 'much', 'higher', 'drag', ',', 'causing', 'the', 'shuttlecock', 'to', 'decelerate', 'more', 'rapidly', '.', 'Shuttlecocks', 'also', 'have', 'a', 'high', 'top', 'speed', 'compared', 'to', 'the', 'balls', 'in', 'other', 'racquet', 'sports', ',', 'making', 'badminton', 'the', 'fastest', 'racquet', 'sport', 'in', 'the', 'world', '.', 'The', 'flight', 'of', 'the', 'shuttlecock', 'gives', 'the', 'sport', 'its', 'distinctive', 'nature', ',', 'and', 'in', 'certain', 'languages', 'the', 'sport', 'is', 'named', 'by', 'reference', 'to', 'this', 'feature', '(', 'e.g.', ',', 'German', 'Federball', ',', 'literally', 'feather-ball', ')', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "var_new = word_tokenize(var)\n",
    "\n",
    "txt_tokenized = WPT.tokenize(var)\n",
    "\n",
    "print(var_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Punctuation based Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~', 'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
      "The\n",
      "shuttlecock\n",
      "feathered\n",
      "informal\n",
      "matches\n",
      "plastic\n",
      "projectile\n",
      "flies\n",
      "differently\n",
      "balls\n",
      "used\n",
      "many\n",
      "sports\n",
      "In\n",
      "particular\n",
      "feathers\n",
      "create\n",
      "much\n",
      "higher\n",
      "drag\n",
      "causing\n",
      "shuttlecock\n",
      "decelerate\n",
      "rapidly\n",
      "Shuttlecocks\n",
      "also\n",
      "high\n",
      "top\n",
      "speed\n",
      "compared\n",
      "balls\n",
      "racquet\n",
      "sports\n",
      "making\n",
      "badminton\n",
      "fastest\n",
      "racquet\n",
      "sport\n",
      "world\n",
      "The\n",
      "flight\n",
      "shuttlecock\n",
      "gives\n",
      "sport\n",
      "distinctive\n",
      "nature\n",
      "certain\n",
      "languages\n",
      "sport\n",
      "named\n",
      "reference\n",
      "feature\n",
      "e.g.\n",
      "German\n",
      "Federball\n",
      "literally\n",
      "feather-ball\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "stop_word_list = list(punctuation)+stop\n",
    "print(stop_word_list)\n",
    "\n",
    "for i in var_new:\n",
    "    if i not in stop_word_list:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Porter Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original Words ['Dancing', 'speaks', 'happilly', 'runs']\n",
      "Stemmed Words ['danc', 'speak', 'happilli', 'run']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "porter_stemmer = PorterStemmer()\n",
    "\n",
    "words = [\"Dancing\" , \"speaks\" , \"happilly\", \"runs\"]\n",
    "\n",
    "stemmed_words = [porter_stemmer.stem(word) for word in words]\n",
    "\n",
    "print(\"original Words\", words)\n",
    "print(\"Stemmed Words\", stemmed_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Snowball & Regexp Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Words: ['running', 'jumping', 'happily', 'foxes']\n",
      "Stemmed words using SnowballStemmer: ['run', 'jump', 'happili', 'fox']\n",
      "Stemmed words using RegexpStemmer: ['runn', 'jump', 'happily', 'foxes']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer, RegexpStemmer\n",
    "\n",
    "s = SnowballStemmer('english')\n",
    "r = RegexpStemmer(\"ing\")\n",
    "\n",
    "words = ['running','jumping', 'happily', 'foxes']\n",
    "\n",
    "stemmed_s = [s.stem(word) for word in words]\n",
    "stemmed_r = [r.stem(word) for word in words]\n",
    "            \n",
    "print(\"Original Words:\", words)\n",
    "print(\"Stemmed words using SnowballStemmer:\", stemmed_s)\n",
    "print(\"Stemmed words using RegexpStemmer:\",stemmed_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lancaster Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Words ['running', 'jumped', 'happily', 'foxes']\n",
      "Stemmed Words ['run', 'jump', 'happy', 'fox']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "\n",
    "stemmer = LancasterStemmer()\n",
    "\n",
    "words = ['running','jumped', 'happily', 'foxes']\n",
    "\n",
    "stemmed_words = [stemmer.stem(word) for word in words] \n",
    "\n",
    "print(\"Original Words\", words)\n",
    "print(\"Stemmed Words\", stemmed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WordNet Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word                Lemma               \n",
      "Good                Good                \n",
      "muffins             muffins             \n",
      "cost                cost                \n",
      "$                   $                   \n",
      "3.88                3.88                \n",
      "in                  in                  \n",
      "New                 New                 \n",
      "York.               York.               \n",
      "Please              Please              \n",
      "buy                 buy                 \n",
      "me                  me                  \n",
      "two                 two                 \n",
      "of                  of                  \n",
      "them.               them.               \n",
      "Thanks              Thanks              \n",
      ".                   .                   \n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "print(\"{0:20}{1:20}\".format(\"Word\",\"Lemma\"))\n",
    "for word in Tokenized:\n",
    "    print(\"{0:20}{1:20}\".format(word,wordnet_lemmatizer.lemmatize(word,pos=\"v\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word                Lemma               \n",
      "The                 The                 \n",
      "flight              flight              \n",
      "of                  of                  \n",
      "the                 the                 \n",
      "shuttlecock         shuttlecock         \n",
      "gives               give                \n",
      "the                 the                 \n",
      "sport               sport               \n",
      "its                 its                 \n",
      "distinctive         distinctive         \n",
      "nature              nature              \n",
      "and                 and                 \n",
      "in                  in                  \n",
      "certain             certain             \n",
      "languages           languages           \n",
      "the                 the                 \n",
      "sport               sport               \n",
      "is                  be                  \n",
      "named               name                \n",
      "by                  by                  \n",
      "reference           reference           \n",
      "to                  to                  \n",
      "this                this                \n",
      "feature             feature             \n",
      "(                   (                   \n",
      "e.g.                e.g.                \n",
      "German              German              \n",
      "Federball           Federball           \n",
      "literally           literally           \n",
      "feather-ball        feather-ball        \n",
      ")                   )                   \n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "sentence = \"The flight of the shuttlecock gives the sport its distinctive nature, and in certain languages the sport is named by reference to this feature (e.g., German Federball, literally feather-ball).\"\n",
    "\n",
    "punctuation = \"?;!.,:\"\n",
    "\n",
    "sentence_words = nltk.word_tokenize(sentence)\n",
    "\n",
    "for word in sentence_words:\n",
    "    if word in punctuation:\n",
    "        sentence_words.remove(word)\n",
    "\n",
    "sentence_words\n",
    "print(\"{0:20}{1:20}\".format(\"Word\",\"Lemma\"))\n",
    "for word in sentence_words:\n",
    "    print(\"{0:20}{1:20}\".format(word,wordnet_lemmatizer.lemmatize(word,pos=\"v\")))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
